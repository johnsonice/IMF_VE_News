{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple topic modelings - with google news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "#from utils import Args\n",
    "import json \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from topic_model_utils import *\n",
    "#model_setup,train_topic_model,eval_topic_model,train_and_eval,hyper_param_permutation,Args,pack_update_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define some global paths and variables  \n",
    "LOAD_EMB = True\n",
    "TUNE = True\n",
    "res_folder = '/data/chuang/news_scrape/data/news_search_res'\n",
    "news_output_p = os.path.join(res_folder,'news_merged.csv')\n",
    "emb_path  = os.path.join(res_folder,'sentence_embeddings.npy')\n",
    "docs_path = os.path.join(res_folder,'docs.npy')\n",
    "topic_model_out_path = os.path.join(res_folder,'topic_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newspaper_name</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_end</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>body</th>\n",
       "      <th>body_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iwnsvg.com</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>Opposition leader: Taiwanese letting Vincies o...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiSWh0d...</td>\n",
       "      <td>ST. VINCENT: – Stakeholders in the agricultura...</td>\n",
       "      <td>2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iwnsvg.com</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>Three artistes bag early Vincy Mas wins – iWit...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiTmh0d...</td>\n",
       "      <td>ST. VINCENT: – Three artistes left the launch ...</td>\n",
       "      <td>3133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iwnsvg.com</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>U.S. to reengage Caribbean, diplomats says – i...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiS2h0d...</td>\n",
       "      <td>CARIBBEAN: – The Barack Obama administration i...</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iwnsvg.com</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>Vincies safe as 27 die in Jamaica shootouts – ...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiTmh0d...</td>\n",
       "      <td>ST. VINCENT:- Vincentian students at the Unive...</td>\n",
       "      <td>3338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iwnsvg.com</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Concerns about approved BLA capitalization pla...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiUmh0d...</td>\n",
       "      <td>Some members of the Building and Loan Associat...</td>\n",
       "      <td>5857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  newspaper_name  time_start    time_end  year  month  day  \\\n",
       "0     iwnsvg.com  2010-05-01  2010-06-01  2010      5    9   \n",
       "1     iwnsvg.com  2010-05-01  2010-06-01  2010      5   10   \n",
       "2     iwnsvg.com  2010-05-01  2010-06-01  2010      5   11   \n",
       "3     iwnsvg.com  2010-05-01  2010-06-01  2010      5   25   \n",
       "4     iwnsvg.com  2013-09-01  2013-10-01  2013      9    1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Opposition leader: Taiwanese letting Vincies o...   \n",
       "1  Three artistes bag early Vincy Mas wins – iWit...   \n",
       "2  U.S. to reengage Caribbean, diplomats says – i...   \n",
       "3  Vincies safe as 27 die in Jamaica shootouts – ...   \n",
       "4  Concerns about approved BLA capitalization pla...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://news.google.com/rss/articles/CBMiSWh0d...   \n",
       "1  https://news.google.com/rss/articles/CBMiTmh0d...   \n",
       "2  https://news.google.com/rss/articles/CBMiS2h0d...   \n",
       "3  https://news.google.com/rss/articles/CBMiTmh0d...   \n",
       "4  https://news.google.com/rss/articles/CBMiUmh0d...   \n",
       "\n",
       "                                                body  body_length  \n",
       "0  ST. VINCENT: – Stakeholders in the agricultura...         2960  \n",
       "1  ST. VINCENT: – Three artistes left the launch ...         3133  \n",
       "2  CARIBBEAN: – The Barack Obama administration i...         3624  \n",
       "3  ST. VINCENT:- Vincentian students at the Unive...         3338  \n",
       "4  Some members of the Building and Loan Associat...         5857  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## raw data exploration \n",
    "df = pd.read_csv(news_output_p)\n",
    "docs = df['body'].tolist() ## get only the body \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUNE = True\n",
      "verbose = True\n",
      "model_name = sentence-transformers/all-MiniLM-L6-v2\n",
      "model_checkpoint = sentence-transformers/all-MiniLM-L6-v2\n",
      "n_neighbors = 15\n",
      "n_components = 5\n",
      "min_cluster_size = 5\n",
      "min_samples = 5\n",
      "min_df = 5\n",
      "nr_topics = auto\n",
      "metric = euclidean\n",
      "calculate_probabilities = False\n",
      "top_n_words = 10\n"
     ]
    }
   ],
   "source": [
    "## define an arg class to read arguments from json\n",
    "args = Args('./args/train_args.json')\n",
    "for attr, value in args.__dict__.items():\n",
    "    print(f\"{attr} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embeding from /data/chuang/news_scrape/data/news_search_res/sentence_embeddings.npy\n",
      "Number of docs: 2971\n"
     ]
    }
   ],
   "source": [
    "#### load sentence embeding model and encode docs\n",
    "if not LOAD_EMB:\n",
    "    print('use model : {}'.format(args.model_name))\n",
    "    sentence_model = SentenceTransformer(args.model_name)\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=True) ## encode sentences \n",
    "    assert len(docs)==len(embeddings)\n",
    "    embeddings = np.array(embeddings)\n",
    "    docs = np.array(docs)\n",
    "    np.save(emb_path,embeddings)\n",
    "    np.save(docs_path,docs)\n",
    "else:\n",
    "    print('Load embeding from {}'.format(emb_path))\n",
    "    embeddings = np.load(emb_path)\n",
    "    docs = np.load(docs_path)\n",
    "    assert len(docs)==len(embeddings)\n",
    "    print('Number of docs: {}'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up parameter search space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## permutate all conbinations \n",
    "train_args_inputs = {\n",
    "            'n_neighbors':[5,10],\n",
    "            'n_components':[3,5],\n",
    "            'min_cluster_size':[5,10],\n",
    "            'min_samples': [5],\n",
    "            'metric':['euclidean'],\n",
    "            'top_n_words':[10],\n",
    "            }\n",
    "train_args_space = hyper_param_permutation(train_args_inputs)\n",
    "#print(train_args_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 20:32:02,833 - BERTopic - Reduced dimensionality\n",
      "2023-11-19 20:32:02,914 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/chuang/Dev/IMF_VE_News/news_scrape/simple_topicmodel/topic_model.ipynb Cell 11\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/data/chuang/Dev/IMF_VE_News/news_scrape/simple_topicmodel/topic_model.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m param \u001b[39m=\u001b[39m train_args_space[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/data/chuang/Dev/IMF_VE_News/news_scrape/simple_topicmodel/topic_model.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m args\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mupdate(param)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/data/chuang/Dev/IMF_VE_News/news_scrape/simple_topicmodel/topic_model.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m topics,probabilities,topic_model \u001b[39m=\u001b[39m train_topic_model(args,docs,embeddings)\n",
      "File \u001b[0;32m/data/chuang/Dev/IMF_VE_News/news_scrape/simple_topicmodel/topic_model_utils.py:170\u001b[0m, in \u001b[0;36mtrain_topic_model\u001b[0;34m(args, docs, embeddings)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_topic_model\u001b[39m(args,docs,embeddings):\n\u001b[1;32m    169\u001b[0m     topic_model \u001b[39m=\u001b[39m model_setup(args)\n\u001b[0;32m--> 170\u001b[0m     topics, probabilities \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39;49mfit_transform(docs,embeddings)\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(probabilities,np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    172\u001b[0m         probabilities \u001b[39m=\u001b[39m probabilities\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.conda/envs/sbert/lib/python3.8/site-packages/bertopic/_bertopic.py:366\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[1;32m    363\u001b[0m     documents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_mappings_by_frequency(documents)\n\u001b[1;32m    365\u001b[0m \u001b[39m# Extract topics by calculating c-TF-IDF\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_topics(documents)\n\u001b[1;32m    368\u001b[0m \u001b[39m# Reduce topics\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnr_topics:\n",
      "File \u001b[0;32m~/.conda/envs/sbert/lib/python3.8/site-packages/bertopic/_bertopic.py:2950\u001b[0m, in \u001b[0;36mBERTopic._extract_topics\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m   2948\u001b[0m documents_per_topic \u001b[39m=\u001b[39m documents\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m], as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mDocument\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin})\n\u001b[1;32m   2949\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_tf_idf_, words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_tf_idf(documents_per_topic)\n\u001b[0;32m-> 2950\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_representations_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_words_per_topic(words, documents)\n\u001b[1;32m   2951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_topic_vectors()\n\u001b[1;32m   2952\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_labels_ \u001b[39m=\u001b[39m {key: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([word[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m values[:\u001b[39m4\u001b[39m]])\n\u001b[1;32m   2953\u001b[0m                       \u001b[39mfor\u001b[39;00m key, values \u001b[39min\u001b[39;00m\n\u001b[1;32m   2954\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_representations_\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.conda/envs/sbert/lib/python3.8/site-packages/bertopic/_bertopic.py:3172\u001b[0m, in \u001b[0;36mBERTopic._extract_words_per_topic\u001b[0;34m(self, words, documents, c_tf_idf)\u001b[0m\n\u001b[1;32m   3170\u001b[0m         topics \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mextract_topics(\u001b[39mself\u001b[39m, documents, c_tf_idf, topics)\n\u001b[1;32m   3171\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model, BaseRepresentation):\n\u001b[0;32m-> 3172\u001b[0m     topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentation_model\u001b[39m.\u001b[39;49mextract_topics(\u001b[39mself\u001b[39;49m, documents, c_tf_idf, topics)\n\u001b[1;32m   3174\u001b[0m topics \u001b[39m=\u001b[39m {label: values[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_n_words] \u001b[39mfor\u001b[39;00m label, values \u001b[39min\u001b[39;00m topics\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   3176\u001b[0m \u001b[39mreturn\u001b[39;00m topics\n",
      "File \u001b[0;32m~/.conda/envs/sbert/lib/python3.8/site-packages/bertopic/representation/_keybert.py:91\u001b[0m, in \u001b[0;36mKeyBERTInspired.extract_topics\u001b[0;34m(self, topic_model, documents, c_tf_idf, topics)\u001b[0m\n\u001b[1;32m     87\u001b[0m topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_candidate_words(topic_model, c_tf_idf, topics)\n\u001b[1;32m     89\u001b[0m \u001b[39m# We calculate the similarity between word and document embeddings and create\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m# topic embeddings from the representative document embeddings\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m sim_matrix, words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_embeddings(topic_model, topics, representative_docs, repr_doc_indices)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Find the best matching words based on the similarity matrix for each topic\u001b[39;00m\n\u001b[1;32m     94\u001b[0m updated_topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_top_words(words, topics, sim_matrix)\n",
      "File \u001b[0;32m~/.conda/envs/sbert/lib/python3.8/site-packages/bertopic/representation/_keybert.py:163\u001b[0m, in \u001b[0;36mKeyBERTInspired._extract_embeddings\u001b[0;34m(self, topic_model, topics, representative_docs, repr_doc_indices)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m\"\"\" Extract the representative document embeddings and create topic embeddings.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mThen extract word embeddings and calculate the cosine similarity between topic\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39membeddings and the word embeddings. Topic embeddings are the average of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m    vocab: The complete vocabulary of input documents\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Calculate representative docs embeddings and create topic embeddings\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m repr_embeddings \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39;49m_extract_embeddings(representative_docs, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    164\u001b[0m topic_embeddings \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mmean(repr_embeddings[i[\u001b[39m0\u001b[39m]:i[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m repr_doc_indices]\n\u001b[1;32m    166\u001b[0m \u001b[39m# Calculate word embeddings and extract best matching with updated topic_embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sbert/lib/python3.8/site-packages/bertopic/_bertopic.py:2797\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[0;34m(self, documents, method, verbose)\u001b[0m\n\u001b[1;32m   2795\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model\u001b[39m.\u001b[39membed_words(documents, verbose)\n\u001b[1;32m   2796\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2797\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49membed_documents(documents, verbose)\n\u001b[1;32m   2798\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong method for extracting document/word embeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2800\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mEither choose \u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m\u001b[39m as the method. \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "param = train_args_space[1]\n",
    "args.__dict__.update(param)\n",
    "topics,probabilities,topic_model = train_topic_model(args,docs,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate through all params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if we wnat to run hpyer tuning\n",
    "if TUNE:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  ## set it to false to avoid warning message \n",
    "    all_res = []\n",
    "    for idx,param in enumerate(tqdm(train_args_space)):\n",
    "        args.__dict__.update(param)\n",
    "        # for attr, value in args.__dict__.items():\n",
    "        #     print(f\"{attr} = {value}\")\n",
    "        try:\n",
    "            topics,probabilities,topic_model = train_topic_model(args,docs,embeddings)\n",
    "            #coherence_scores,outlier_percent,n_topics,diversity_score = train_and_eval(args,docs,embeddings)\n",
    "            coherence_scores,outlier_percent,n_topics,diversity_score = eval_topic_model(docs,topics,probabilities,\n",
    "                                                                                            topic_model,n_workers=16)\n",
    "        except Exception as e:\n",
    "            print('-- Error -- \\n{}\\n{}'.format(param,e))\n",
    "            coherence_scores,outlier_percent,n_topics,diversity_score = (None,None,None,None)\n",
    "        res_dict = pack_update_param(param,coherence_scores,outlier_percent,n_topics,diversity_score)\n",
    "        all_res.append(res_dict)\n",
    "        if args.verbose:\n",
    "            print(res_dict)\n",
    "\n",
    "    ## print out results \n",
    "    res_df = pd.DataFrame(all_res)\n",
    "    res_df = res_df.sort_values(by='coherence', ascending=False)\n",
    "    best_param = res_df.iloc[0].to_dict()\n",
    "    print(best_param)\n",
    "    res_df.head()\n",
    "else:\n",
    "    best_param={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrain model with best param "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.__dict__.update(best_param)\n",
    "args.verbose=True\n",
    "args.TUNE=False\n",
    "topics,probabilities,topic_model = train_topic_model(args,docs,embeddings)\n",
    "topic_model.save(topic_model_out_path, save_embedding_model=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Topic model visual evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
