{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cf19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "sys.path.insert(0, '../libs')\n",
    "# Import SimpleLLMAgent from libs directory\n",
    "from llm_factory_openai import SimpleLLMAgent\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import io\n",
    "# Import our general LLM factory\n",
    "from llm_factory_general import (\n",
    "    GeneralLLMFactory,\n",
    "    create_openai_factory,\n",
    "    create_google_gemini_factory,\n",
    "    create_anthropic_factory,\n",
    "    create_openai_compatible_factory\n",
    ")\n",
    "from utils import download_hf_model\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')\n",
    "api_key = os.getenv(\"huggingface_token\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"huggingface_token not found in environment variables. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95fde5",
   "metadata": {},
   "source": [
    "#### Test call closed source model api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcddf293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== General LLM Factory Examples ===\n",
      "OpenAI test result: Connection successful\n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=== General LLM Factory Examples ===\")\n",
    "openai_factory = create_openai_factory(model_name='gpt-4.1',\n",
    "                                        temperature=0.0,\n",
    "                                        max_tokens=8000,\n",
    "                                        )\n",
    "result = openai_factory.test_connection()\n",
    "print(f\"OpenAI test result: {result}\")\n",
    "messages = [{\"role\": \"user\", \"content\": 'hi'}]\n",
    "response = openai_factory.get_response_content(messages,response_model=str)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e1e59",
   "metadata": {},
   "source": [
    "Test local SGLang API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705d5323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen3-4B already exists at /ephemeral/home/xiong/data/hf_cache/Qwen/Qwen3-4B, skipping download\n",
      "Model Qwen/Qwen3-8B already exists at /ephemeral/home/xiong/data/hf_cache/Qwen/Qwen3-8B, skipping download\n"
     ]
    }
   ],
   "source": [
    "## download models\n",
    "model_name_list = ['Qwen/Qwen3-4B','Qwen/Qwen3-8B']\n",
    "for model_name in model_name_list:\n",
    "    # Create the target directory path\n",
    "    target_dir = '/ephemeral/home/xiong/data/hf_cache/' + model_name\n",
    "    # Check if model already exists before downloading\n",
    "    if os.path.exists(target_dir) and os.listdir(target_dir):\n",
    "        print(f\"Model {model_name} already exists at {target_dir}, skipping download\")\n",
    "    else:\n",
    "        print(f\"Downloading model {model_name}...\")\n",
    "        download_hf_model(model_name, target_dir, hf_token=os.getenv('huggingface_token'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f5ac1",
   "metadata": {},
   "source": [
    "### SGLang offling inference \n",
    "- https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee7460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch the offline engine\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import sglang as sgl\n",
    "\n",
    "from sglang.srt.conversation import chat_templates\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if is_in_ci():\n",
    "    import patch\n",
    "else:\n",
    "    import nest_asyncio\n",
    "\n",
    "    nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3431b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Qwen/Qwen3-8B'\n",
    "model_path = os.path.join('/ephemeral/home/xiong/data/hf_cache','Qwen/Qwen3-8B')\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82f5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for sgl.Engine corresponding to the launch_server CLI options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26b88d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.83it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  2.33it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:03,  1.30it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:01,  1.95it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:01,  1.79it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.79it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:01,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.89it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.40it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.83it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.25it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  2.61it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  2.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.40it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.45it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  2.26it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:01<00:00,  2.02it/s].23s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.76it/s].42it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  2.06it/s].85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  2.14it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.87it/s]\n",
      "\n",
      "Capturing batches (bs=1 avail_mem=4.81 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:11<00:00,  2.07it/s]]\n",
      "Capturing batches (bs=1 avail_mem=1.68 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:14<00:00,  1.63it/s]]\n",
      "Capturing batches (bs=1 avail_mem=4.25 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:14<00:00,  1.56it/s] \n",
      "Capturing batches (bs=1 avail_mem=3.69 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:15<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model path {model_path} does not exist. Please download the model first.\")\n",
    "engine_args = {\n",
    "    \"model_path\": model_path,                # --model-path\n",
    "    #\"port\": 8100,                            # --port\n",
    "    \"dtype\": \"bfloat16\",                     # --dtype\n",
    "    #\"api_key\": \"abc\",                        # --api-key\n",
    "    \"context_length\": 8192,                  # --context-length\n",
    "    #\"served_model_name\": \"Qwen/Qwen3-8B\",    # --served-model-name\n",
    "    #\"allow_auto_truncate\": True,             # --allow-auto-truncate\n",
    "    \"constrained_json_whitespace_pattern\": r\"[\\n\\t ]*\",  # --constrained-json-whitespace-pattern\n",
    "    \"mem_fraction_static\": 0.9,              # --mem-fraction-static\n",
    "    \"dp_size\": 4,                            # --dp_size\n",
    "    \"grammar_backend\":\"xgrammar\",\n",
    "    # \"reasoning_parser\": \"qwen3\"              # --reasoning-parser # thes works very strangely \n",
    "}\n",
    "\n",
    "# Instantiate the engine with these arguments\n",
    "llm = sgl.Engine(**engine_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235635c",
   "metadata": {},
   "source": [
    "#### Simple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcbee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello, my name is\n",
      "Generated text:  Alex. I am a 23-year-old student who is currently studying for a degree in computer science. I am interested in learning more about the field of artificial intelligence and its applications. I have a basic understanding of programming, but I would like to improve my skills in this area. I am also interested in exploring the ethical implications of AI and how it can be used responsibly. I am looking for a mentor who can guide me in my studies and help me understand the concepts better. I am open to learning from someone with experience in AI, machine learning, or related fields. I am available for mentorship sessions on weekends or evenings\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  the head of state and head of government of the United States, and the leader of the executive branch of the federal government. The president is also the commander-in-chief of the United States Armed Forces. The president is elected to a four-year term by the people of the United States through the Electoral College, and is the only federal official who is directly elected by the people. The president is the only federal official who is directly elected by the people. The president is the only federal official who is directly elected by the people. The president is the only federal official who is directly elected by the people. The president is the only federal official who\n",
      "Prompt: The capital of France is\n",
      "Generated text:  Paris. The capital of Italy is Rome. The capital of Spain is Madrid. The capital of Germany is Berlin. The capital of the Netherlands is Amsterdam. The capital of Belgium is Brussels. The capital of Portugal is Lisbon. The capital of Switzerland is Bern. The capital of Austria is Vienna. The capital of Poland is Warsaw. The capital of Czech Republic is Prague. The capital of Hungary is Budapest. The capital of Sweden is Stockholm. The capital of Norway is Oslo. The capital of Denmark is Copenhagen. The capital of Finland is Helsinki. The capital of Iceland is Reykjavik. The capital of Latvia is Riga. The\n",
      "Prompt: The future of AI is\n",
      "Generated text:  a topic that has sparked a lot of discussion and debate. As we continue to develop and refine artificial intelligence, it's important to consider the potential benefits and risks that come with it. One of the key areas of focus is the ethical implications of AI, including issues such as bias, privacy, and accountability. Another important aspect is the impact of AI on the job market, as automation has the potential to both create and eliminate jobs. Additionally, there are concerns about the security and safety of AI systems, as well as the potential for misuse by malicious actors. It's also important to consider the regulatory frameworks that may be needed to govern the\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = {\"temperature\": 0.01, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c74761",
   "metadata": {},
   "source": [
    "#### Prompts with structured outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb391e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: What is the capital of China?\n",
      "{ \"name\": \"Beijing\", \"opulation\": 2154 }\n",
      "===============================\n",
      "Prompt: What is the capital of Japan?\n",
      "{ \"name\": \"Tokyo\", \"opulation\": 37400068 }\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"What is the capital of Japan?\",\n",
    "]\n",
    "\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str = Field(..., pattern=r\"^\\w+$\", description=\"Name of the capital city\")\n",
    "    opulation: int = Field(..., description=\"Population of the capital city\")\n",
    "\n",
    "sampling_params = {\"temperature\": 0.1,\"top_p\": 0.95,\"json_schema\": json.dumps(CapitalInfo.model_json_schema())}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\")  # validate the output by the pydantic model\n",
    "    print(output[\"text\"])\n",
    "    # capital_info = CapitalInfo.model_validate_json(output[\"text\"])\n",
    "    # print(f\"Validated output: {capital_info.model_dump_json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11793494",
   "metadata": {},
   "source": [
    "#### Apply Proper Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b327ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Here is the information of the capital of France.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "{\n",
      "  \"name\": \"Paris\",\n",
      "  \"opulation\": 2148274\n",
      "  \t\t\n",
      "\t\n",
      "}\n",
      "===============================\n",
      "Prompt: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Here is the information of the capital of China .\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "{\n",
      "  \"name\": \"Beijing\",\n",
      "  \"opulation\": 21540000\n",
      "  \n",
      "  \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Make API request\n",
    "messages = [\n",
    "    [{\"role\": \"system\",\n",
    "      \"content\":\"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is the information of the capital of France.\\n\",\n",
    "    }],\n",
    "    [{\"role\": \"system\",\n",
    "      \"content\":\"You are a helpful assistant.\"},\n",
    "     {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is the information of the capital of China .\\n\",\n",
    "    }],\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\")  # validate the output by the pydantic model\n",
    "    print(output[\"text\"])\n",
    "    # capital_info = CapitalInfo.model_validate_json(output[\"text\"])\n",
    "    # print(f\"Validated output: {capital_info.model_dump_json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ddea81",
   "metadata": {},
   "source": [
    "#### Use Openai to call local llm with sglang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bbfb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI test result: Hello! ðŸ˜Š How can I assist you today? Whether you have questions, need help with something, or just want to chat, I'm here for you! What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "from llm_factory_openai import SimpleLLMAgent\n",
    "\n",
    "local_model_args = {\"model\":\"Qwen/Qwen3-8B\",\n",
    "                    \"base_url\":\"http://localhost:8101/v1\",\n",
    "                    \"temperature\":0.1,\n",
    "                    \"api_key\":\"abc\"\n",
    "                    }\n",
    "openai_agent = SimpleLLMAgent(**local_model_args)\n",
    "\n",
    "result = openai_agent.test_connection()\n",
    "print(f\"OpenAI test result: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd5c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "## send messages to sglang server; try to response in json format\n",
    "messages = [{\"role\": \"user\", \"content\": 'hi'}]\n",
    "response = openai_agent.get_response_content(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7ce900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output: {\n",
      "  \"country\": \"Japan\",\n",
      "  \"capital\": \"Tokyo\",\n",
      "  \"population_millions\": 126.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define a Pydantic model for structured output\n",
    "class CountryInfo(BaseModel):\n",
    "    country: str = Field(..., description=\"Country name\")\n",
    "    capital: str = Field(..., description=\"Capital city\")\n",
    "    population_millions: float = Field(..., description=\"Population in millions\")\n",
    "\n",
    "# Prepare a prompt asking for structured information\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that provides information in JSON format.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Please provide the following information about Japan in JSON format: \"\n",
    "            \"country name, capital city, and population in millions.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get structured response using the Pydantic model\n",
    "structured_result = openai_agent.get_response_content(messages, response_format=CountryInfo)\n",
    "print(\"Structured output:\", structured_result.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542162db",
   "metadata": {},
   "source": [
    "#### Test using Openai competable batch job with SGLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195569e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.lib._parsing._completions import type_to_response_format_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90107cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_to_response_format_param(CountryInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_tasks(batch_messages, output_format_type,model_name,task_id):\n",
    "    \n",
    "    output_json_schema = type_to_response_format_param(output_format_type)\n",
    "    \n",
    "    tasks = []\n",
    "    for messages in batch_messages:\n",
    "        task = {\n",
    "            \"custom_id\": f\"task-{task_id}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model_name,\n",
    "                \"messages\": messages,\n",
    "                \"response_format\": output_json_schema, #{\"type\": \"json_object\"}, #output_json_schema, output format affects output speed\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_path = \"/ephemeral/home/xiong/data/Fund/Factiva_News/temp/batch.jsonl\"\n",
    "batch_messages = [messages] *100\n",
    "tasks = create_batch_tasks(batch_messages, output_format_type=CountryInfo,model_name='Qwen/Qwen3-8B',task_id='test')\n",
    "jsonl_data = '\\n'.join([json.dumps(t) for t in tasks])\n",
    "open(batch_file_path, 'w').write(jsonl_data)\n",
    "print(f\"Created {len(tasks)} batch tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637fd51",
   "metadata": {},
   "source": [
    "- batch file process, currently not supported : https://github.com/sgl-project/sglang/issues/7427\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dbcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factiva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
